---
title: "Managing files and downloads"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Managing files and downloads}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```



```{r setup}
library(castarter2)
```




One of the first issues that appear when starting a text mining or web scraping project relates to the issue of managing files and folder. `castarter2` defaults to an opinionated folder structure that should work for most projects. It also facilitates downloading files (skipping previously downloaded files) and ensuring consistent and unique matching between a downloaded html, its source url, and data extracted from them. Finally, it facilitates archiving and backuping downloaded files and scripts.


## Getting started

In this vignette, I will outline some of the basic steps that can be used to extract and process the contents of press releases from a number of institutions of the European Union. When text mining or scraping, it is common to gather quickly many thousands of file, and keeping them in good order is fundamental, particularly in the long term. 

A preliminary suggestion: depending on how you usually work and keep your files backed-up it may make sense to keep your scripts in a folder that is live-synced (e.g. with services such as Dropbox, Nextcloud, or Google Drive). It however rarely make sense to live-sync tens or hundreds of thousands of files as you proceed with your scraping. You may want to keep this in mind as you set the `base_folder` with `cas_set_options()`. I will keep in the current working directory here for the sake of simplicity, but there are no drawback in having scripts and folders in different locations. 


```{r}

cas_set_options(base_folder = "castarter_data",
                project = "european_union",
                website = "european_commission"
)

```
Assuming that my project on the European Union involves text mining the website of the European Council, the European Commission, and the European Parliament, the folder structure may look something like this:


```{r eval = TRUE, include = FALSE}
fs::dir_create(path = fs::path(cas_get_options()$base_folder,
                               cas_get_options()$project,
                               cas_get_options()$website))

fs::dir_create(path = fs::path(cas_get_options()$base_folder,
                               cas_get_options()$project,
                               "european_parliament"))

fs::dir_create(path = fs::path(cas_get_options()$base_folder,
                               cas_get_options()$project,
                               "european_council"))

fs::dir_tree(cas_get_options()$base_folder)
```


In brief, `castarter_data` is the base folder where I can store all of my text mining projects. `european_union` is the name of the project, while all others are the names of the specific websites I will source. Folders will by created automatically as needed when you start downloading files. 


## Downloading index files

In text mining a common scenario involves first downloading web pages containing lists of urls to the actual posts we are interested in. In the case of the European Commission, these would probably the pages in the "[news section](https://ec.europa.eu/commission/presscorner/home/en)". By clicking on the the numbers at the bottom of the page, we get to see direct links to the subsequent pages listing all posts. 

These URLs look something like this:

```{r}
index_urls <- paste0("https://ec.europa.eu/commission/presscorner/home/en?pagenumber=", 1:10)

index_urls %>% 
  knitr::kable()
```

Let's say we're content with downloading the latest 100 posts. 

```{r}

```


## European Parliament

Recent press releases published on the official website of the [European Parliament](https://www.europarl.europa.eu/news/en/press-room). 



